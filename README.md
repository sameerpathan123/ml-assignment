# How the Depth and Width of Multilayer Perceptrons Affect Learning

This repository contains a hands-on tutorial exploring how the architecture of Multilayer Perceptrons (MLPs) — specifically the depth (number of layers) and width (number of neurons per layer) — impacts learning performance, generalization, and overfitting. It is designed as an educational resource for students, researchers, and machine learning practitioners.

---

##  Tutorial Overview

In this notebook, you will learn:
- What Multilayer Perceptrons (MLPs) are and how they function
- The difference between depth and width in neural network architecture
- How model capacity impacts bias, variance, underfitting, and overfitting
- How to visualize training and validation performance
- Practical experimentation with varying architectures using synthetic data

This tutorial is implemented in Python using scikit-learn, matplotlib, and TensorFlow/Keras.

---

##  How to Reproduce This Tutorial

To reproduce the steps in the tutorial on your local machine:

